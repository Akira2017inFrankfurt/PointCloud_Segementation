{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "介绍如何将seq2seq模型转换为PyTorch可用的前端混合Torch脚本。\n",
    "\n",
    "### 1. 混合前端\n",
    "\n",
    "PyTorch提供了将即时模式的代码增量转换为Torch脚本的机制，\n",
    "torch脚本是一个在python中的静态可分析和可优化的子集。\n",
    "torch用它来在Python运行时独立进行深度学习。\n",
    "\n",
    "\n",
    "Torch中torch.jit模块可以找到将即时Pytorch程序转换为Torch脚本的API。\n",
    "这个模块有两个核心模式用于将即时模式转换为Torch脚本图形的表示：\n",
    "跟踪tracing + 脚本化 scripting\n",
    "\n",
    "torch.jit.trace函数接受一个模块/一个函数和一组示例的输入\n",
    "然后通过函数或模块运行输入示例，同时跟踪遇到的计算步骤\n",
    "然后输出一个可以展示跟踪流程的基于图形的函数。\n",
    "\n",
    "跟踪tracing对于不依赖于数据的控制流的直接的函数和模块非常有用，比如标准的CNN。\n",
    "\n",
    "\n",
    "### 2.预备环境\n",
    "\n",
    "导入模块。\n",
    "如使用自己的模型，需要保证MAX_LENGTH常量设置正确。\n",
    "ps 这个常量定义了在训练过程中允许的最大的句子长度以及模型能够产生的最大句子长度输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "MAX_LENGTH = 10 # maximum sentence length\n",
    "\n",
    "# 默认的词向量\n",
    "PAD_token = 0 # used for padding short sentence\n",
    "SOS_token = 1 # Start-of-sentence token\n",
    "EOS_token = 2 # End-of-sentence token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 模型概述\n",
    "\n",
    "使用sequence-to-sequence模型。\n",
    "这种模型的输入是可变长度序列的情况，输出是可变长度序列，不一定是一对一输入影射。\n",
    "seq2seq模型由两个RNN组成：encode + decoder\n",
    "\n",
    "##### （1）编码器Encoder\n",
    "编码器RNN在输入语句中每次迭代一个标记，比如单词。\n",
    "每次步骤输出一个“输出”向量 + 一个“隐藏状态”向量\n",
    "\n",
    "隐藏状态向量在之后则传递到下一个步骤。\n",
    "同时记录输出向量。\n",
    "\n",
    "编码器将序列中每个坐标代表的文本转换为高维空间中的一组坐标。\n",
    "解码器将使用这些作为为给定的任务生成有意义的输出。\n",
    "\n",
    "##### （2） 解码器Decoder\n",
    "\n",
    "解码器RNN以逐个令牌的方式生成相应语句。\n",
    "它使用来自于编码器的文本向量和内部隐藏状态来生成序列中的下一个单词。\n",
    "它继续生成单词，直到输出表示句子结束的EOS语句。\n",
    "\n",
    "在解码器中使用专注机制attention mechanism来帮助它在输入的某些部分输出时“保持专注”。\n",
    "在我们的模型中，实现了“全局关注Global attention”模块，并将其作为解码模型中的子模块。\n",
    "\n",
    "### 4. 数据处理\n",
    "\n",
    "在训练之前建立的模型词汇表中的每个单词都影射到一个整数索引。\n",
    "我们使用Voc对象来包含从单词到索引的映射，以及词汇表中的单词总数。\n",
    "我们将在运行模型之前加载对象。\n",
    "\n",
    "此外，为了能够评估，必须提供一个字符串输入的工具。\n",
    "normalizeString函数将字符串中的所有字符串转换成小写，并删除所有非字母字符。\n",
    "indexesFromSentence函数接受一个单词的句子，并返回相应的单词索引序列。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Voc:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token:'PAD', SOS_token:'SOS', EOS_token:'EOS'}\n",
    "        self.num_words = 3 # 统计SOS， EOS， PAD\n",
    "        \n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(''):\n",
    "            self.addWord(word)\n",
    "            \n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "    # remove words below a certain count threshold\n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "        keep_words = []\n",
    "        for k, v in self.word2count.item():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "                \n",
    "        print('keep_words{}/{} = {:.4f}'.format(\n",
    "            len(keep_words), \n",
    "            len(self.word2index),\n",
    "            len(keep_words) / len(self.word2index)\n",
    "        ))\n",
    "        \n",
    "        # reinitialize dictionaries\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token:'PAD', SOS_token:'SOS', EOS_token:'EOS'}\n",
    "        self.num_words = 3 # 默认统计令牌\n",
    "        for word in keep_words:\n",
    "            self.addWord(word)\n",
    "            \n",
    "# 小写并删除非字母字符\n",
    "def normalizaString(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "# 使用字符串句子，返回单词索引的句子\n",
    "def indexesFromSentence(voc, sentence):\n",
    "    return [voc.word2index[word] for word in sentence.split('')] + [EOS_token]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 定义编码器\n",
    "\n",
    "通过torch.nn.GRU模块实现编码器RNN。\n",
    "\n",
    "接受一批语句（嵌入单词的向量）的输入\n",
    "它在内部遍历这些句子\n",
    "每次一个标记，计算隐藏状态。\n",
    "\n",
    "将这个模块初始化为双向的，相当于拥有两个独立的GRUs\n",
    "一个按照时间顺序遍历序列\n",
    "另一个按照相反的顺序遍历序列\n",
    "最终返回两个GRUs之和。\n",
    "\n",
    "模型是按照批处理进行训练的\n",
    "所以在EncoderRNN模型的forward函数需要一个填充的输入批处理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "        \n",
    "        # 初始化GRU input_size hidden_size等参数都设置为hidden_size\n",
    "        # 因为我们输入的大小是一个有多个特征的词向量 == hidden_size\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
    "                         dropout=(0 if n_layers==1 else dropout),\n",
    "                         bidirectional=True)\n",
    "    \n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        # 将单词索引转换为向量\n",
    "        embedded = self.embedding(input_seq)\n",
    "        # 为RNN模块填充批次序序列\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_length)\n",
    "        # 正向通过GRU\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        # 打开填充\n",
    "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        # 将双向GRU的输出结果加和\n",
    "        outputs = outputs[:,:, :self.hidden_size] + outputs[:, :, self.hidden_size:]\n",
    "        # 返回输出以及最终的隐藏状态\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 定义解码器的注意力模块\n",
    "\n",
    "注意力模块Attn，将用作解码器模型的子模块。\n",
    "取当前解码器RNN输出和整个编码器的输出，并返回关注点“能值”energies。\n",
    "这个关注能值张量attension energies tensor与编码器输出的大小相同\n",
    "两者最终相乘，得到一个加权张量\n",
    "\n",
    "其最大值表示在特定时间步长解码的查询语句最重要的部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Luong的注意力层\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        \n",
    "        self.method = method\n",
    "        if self.method not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(self.method, \"is not an appropriate attention method\")\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "            \n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "    \n",
    "    def general_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(encoder_output)\n",
    "        return torch.sum(hidden * energy, dim=2)\n",
    "    \n",
    "    def concat_score(self, hidden, encoder_output):\n",
    "        energy - self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
    "        return torch.sum(self.v * energy, dim=2)\n",
    "    \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # 根据给定的计算方法计算注意力权重/能量\n",
    "        if self.method == 'general':\n",
    "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'concat':\n",
    "            attn_erergies = self.concat_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'dot':\n",
    "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "            \n",
    "        # 转制max_length和batch_size维度\n",
    "        attn_energies = attn.energies.t()\n",
    "        \n",
    "        # 返回softmax归一化概率分数（增加维度）\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 定义解码器\n",
    "\n",
    "还是使用nn.GRU模块作为解码器RNN。\n",
    "但是这次使用的是单向的GRU。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "        \n",
    "        # 保持参考\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # 定义层\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers==1 else dropout))\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        self.attn = Attn(attn_model, hidden_size)\n",
    "        \n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        # 这步只运行一次\n",
    "        # 获取当前输入字对应的向量映射\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        # 通过单向GRU转发\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "        # 通过当前GRU计算注意力权重\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        # 注意力权重乘以编码器输出以获得新的加权和 上下文向量\n",
    "        context = attn_weights.bmm(encoder_output.transpose(0, 1))\n",
    "        # 使用Luong公式5来连接加权上下文向量和GRU输出\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        # Luong公式6 来预测下一个单词\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        # 返回输出和最终的隐藏状态\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
    "    # 格式化输入句子作为批处理\n",
    "    # words -> indexes\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
    "    # 创建长度张量\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    # 转置批量的维度以匹配模型的期望\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "    # 使用适当的设备\n",
    "    input_batch = input_batch.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    # 用earcher解码句子s\n",
    "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
    "    # indexes -> words\n",
    "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
    "    return decoded_words\n",
    "\n",
    "\n",
    "# 评估来自用户输入的输入(stdin)\n",
    "def evaluateInput(encoder, decoder, searcher, voc):\n",
    "    input_sentence = ''\n",
    "    while(1):\n",
    "        try:\n",
    "            # 获取输入的句子\n",
    "            input_sentence = input('> ')\n",
    "            # Check if it is quit case\n",
    "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
    "            # 规范化句子\n",
    "            input_sentence = normalizeString(input_sentence)\n",
    "            # 评估句子\n",
    "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "            # 格式化和打印回复句\n",
    "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "            print('Bot:', ' '.join(output_words))\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word.\")\n",
    "\n",
    "# 规范化输入句子并调用evaluate()\n",
    "def evaluateExample(sentence, encoder, decoder, searcher, voc):\n",
    "    print(\"> \" + sentence)\n",
    "    # 规范化句子\n",
    "    input_sentence = normalizeString(sentence)\n",
    "    # 评估句子\n",
    "    output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "    output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "    print('Bot:', ' '.join(output_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
